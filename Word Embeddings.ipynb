{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings\n",
    "\n",
    "Word embeddings, numerical representations of words based on deep learning neural networks, have revolutionized natural language processing over the last eight years. Literally, every application of NLP has been transformed by this technology. We'll start modestly, using the numerical notion of distance to understand political speech. \n",
    "\n",
    "Our goal is multi-step: \n",
    "\n",
    "1. Scrape all speeches from the DNC and RNC national conventions. Good news, that's already done.\n",
    "1. Convert each speech giver in a numeric value, based on the words they delivered at the convention.\n",
    "1. Measure their distance to the speeches of Donald Trump and Joe Biden.\n",
    "1. Visualize speech givers along these dimensions.\n",
    "\n",
    "A good reference for word embeddings in spaCy can be found in [the documentation](https://spacy.io/usage/vectors-similarity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parser up here, so we don't keep reloading it as \n",
    "# we run cells down below.\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the Data\n",
    "\n",
    "Let's query the database (a copy of which is up on Moodle) and create a dictionary to store the data. The key should be the speaker and the value should be a string containing _everything_ they said at the convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"ConventionSpeeches.db\")\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query to pull speaker, party and text from the database\n",
    "convention_data = cur.execute(\"\"\"\n",
    "\n",
    "                                \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = defaultdict(str)\n",
    "party_lu = defaultdict(str)\n",
    "\n",
    "for item in convention_data :\n",
    "    speaker, party, text = item\n",
    "    \n",
    "    speeches[speaker] = \" \".join([speeches[speaker],text]).strip() \n",
    "    party_lu[speaker] = party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people appeared in both conventions via video clips. Let's make sure they all have the correct party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_lu[\"Donald Trump\"] = \"Republican\"\n",
    "party_lu[\"Andrew Cuomo\"] = \"Democratic\"\n",
    "party_lu[\"Joe Biden\"] = \"Democratic\"\n",
    "party_lu[\"Nancy Pelosi\"] = \"Democratic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove any speaker who spoke fewer than `length_cutoff` words at the convention or whose name we don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_cutoff = 100\n",
    "to_remove = []\n",
    "num_removed = 0\n",
    "num_kept = 0\n",
    "\n",
    "for speaker, text in speeches.items() :\n",
    "    \n",
    "    # Get the length of the speaker's text\n",
    "    \n",
    "    # if the text is shorter than length_cutoff,\n",
    "    # add that speaker to `to_remove`\n",
    "    \n",
    "    # Update your kept and removed counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the speakers who don't fit our filtering criteria\n",
    "for speaker in to_remove :\n",
    "    del speeches[speaker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Numeric Vector\n",
    "\n",
    "Now let's turn each speaker into a numeric vector. There are many ways we could potentially do this. We'll follow one of the most straightforward. We'll do some cleaning, convert each word of their speech into a vector, and average those vectors. \n",
    "\n",
    "We'll use the spaCy library's vectorization of words. Note that the small library (`en_core_web_sm`), doesn't include the word vectorization, so you'll need to have downloaded either the medium or large model. You can do this at the command line by running `python -m spacy download en_core_web_md`. The large model is _large_ -- it'll take a long time to download.\n",
    "\n",
    "We're going to take these averages, but let's first play around with one person's speech and the tokens from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = speeches[\"Michelle Obama\"]\n",
    "tokens = nlp(text)\n",
    "token = tokens[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token)\n",
    "print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over all the tokens in Michelle Obama's speeches and build an average word vector for her. I've set up an empty numpy array of the proper length. Let's sum up the vectors in that score vector. If you divide by the number of tokens you'll get the average word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.zeros(300)\n",
    "token_count = 0\n",
    "\n",
    "for token in tokens :\n",
    "    # add the vector to the score\n",
    "    \n",
    "    # update the token count\n",
    "    \n",
    "score = score/token_count\n",
    "\n",
    "# Look at the sum of the score. We'll compare this to a different technique.\n",
    "print(sum(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this more quickly by taking advantage of list comprehensions (making a list of vectors) and simply applying `np.mean` across the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_list = [token.vector for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_vector = np.mean(word_vector_list, axis=0)\n",
    "\n",
    "# check that the sum is the same as the other way\n",
    "print(average_word_vector.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be the same as the value above if you've done it correctly.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's calculate these average word vectors for each speaker. Iterate through the `speeches` object and fill in the scores dictionary with the average word vector for the speaker. It might be helpful to do some cleaning of the speeches, to try to focus on words that carry semantic meaning. There are a variety of things we could try: \n",
    "\n",
    "* Removing stopwords using the `token.is_stop` attribute in spaCy.\n",
    "* Removing punctuation using `token.is_punct`\n",
    "* Keeping only certain parts of speech (like nouns and/or verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(np.array)\n",
    "\n",
    "for speaker, text in speeches.items() :\n",
    "\n",
    "    tokens = nlp(text)\n",
    "    \n",
    "    # Use this space to do some cleaning of the tokens. We'll run \n",
    "    # this code with and without various cleaning approaches, trying\n",
    "    # to find a version that gives us sensible output. \n",
    "    \n",
    "    # write code to calculate the average word vector for the speaker.\n",
    "    \n",
    "    \n",
    "    # store that average word vector in the `scores` object. Replace\n",
    "    # the right-hand side below.\n",
    "    scores[speaker] = np.zeros(300)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Distance\n",
    "\n",
    "Now we'll create two numpy arrays that measure the distance between each speaker and the presidental candiates. For reasons we'll discuss in class, the cosine similariy is the overwhelming choice for text data. The cosine distance measures the angle between the vectors, disregarding the magnitude of the vectors. As you may recall from high school algebra, one formula for the cosine, and the one we'll use, is this: \n",
    "\n",
    "$$\n",
    "    \\cos(a,b) = \\frac{a \\cdot b}{||a||\\cdot||b||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(a,b) :\n",
    "    dist = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_dists = np.zeros(len(scores))\n",
    "biden_dists = np.zeros(len(scores))\n",
    "speakers = []\n",
    "party = []\n",
    "\n",
    "for idx, speaker in enumerate(scores) :\n",
    "    this_vec = scores[speaker]\n",
    "    speakers.append(speaker)\n",
    "    party.append(party_lu[speaker])\n",
    "    \n",
    "    trump_dists[idx] = cosine_dist(scores[\"Donald Trump\"],this_vec)\n",
    "    biden_dists[idx] = cosine_dist(scores[\"Joe Biden\"],this_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy includes a similarity measure based on the cosine distance. You can feed two tokens in and receive the similarity score between them. How could you potentially use this functionality to measure the similarity between speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp(\"young\").similarity(nlp(\"old\")))\n",
    "print(nlp(\"dog\").similarity(nlp(\"cat\")))\n",
    "print(nlp(\"dog\").similarity(nlp(\"justice\")))\n",
    "print(nlp(\"inequality\").similarity(nlp(\"justice\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize these distances to get a sense of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib histogram\n",
    "plt.hist(trump_dists, \n",
    "         color = 'red', \n",
    "         edgecolor = 'black',\n",
    "         bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib histogram\n",
    "plt.hist(biden_dists, \n",
    "         color = 'blue', \n",
    "         edgecolor = 'black',\n",
    "         bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both speakers, most similarities are quite high; the values range from zero to one. Let's figure out which speakers are closest to both Trump and Biden. One way to do this is to put everything in a data frame and do some sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(list(zip(speakers,party, trump_dists.tolist(),biden_dists.tolist())),\n",
    "                         columns=[\"Speaker\",\"Party\",\"TrumpDist\",\"BidenDist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Far from Trump\n",
    "distances.sort_values(\"TrumpDist\").head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close to Trump\n",
    "distances.sort_values(\"TrumpDist\").tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Far from Biden\n",
    "distances.sort_values(\"BidenDist\").head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close to Biden\n",
    "distances.sort_values(\"BidenDist\").tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the speakers who are close and far from the presidential candidates. Do these make sense to you? Play around with the filtering options at the end of the \"Creating a Numeric Vector\" section. What combinations seem to make the most sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Distances\n",
    "\n",
    "Most speakers are pretty similar to each other. Let's make a plot of the distances to Trump and Biden to get a sense of the correlational and overall space of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.scatter(trump_dists,biden_dists , c='blue', alpha=0.2, edgecolors='none')\n",
    "ax.set_xlabel(\"Trump Distances\")\n",
    "ax.set_ylabel(\"Biden Distances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Engines\n",
    "\n",
    "Brenden asked me to post this assignment \"as early as feasible\" so that he could use these ideas to recommend songs. Here we are, last day of classes, and I'm posting this exercise. Sorry Brenden!\n",
    "\n",
    "But the ideas in this exercise can work directly for recommendation engines. Who are the five speakers who are most similar to Biden? How could you use this for song lyrics?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
